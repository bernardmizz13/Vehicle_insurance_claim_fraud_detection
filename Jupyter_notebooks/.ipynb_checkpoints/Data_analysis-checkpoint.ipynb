{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized logger\n",
      "Initialized logger 2023-09-22_19_43_46_437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berna\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.tree.export module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.tree. Anything that cannot be imported from sklearn.tree is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# to retrieve files\n",
    "import os.path\n",
    "# for logging purposes\n",
    "import logging\n",
    "# for mathematical calculations on lists and dataframes\n",
    "import numpy as np\n",
    "# for CSV and dataframe operations\n",
    "import pandas as pd\n",
    "# for plots\n",
    "import matplotlib.pyplot as plt\n",
    "# for mathematical calculations\n",
    "import math\n",
    "# import the label encoder from sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# for training and testing split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# for the decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# to export and save it to JPG later on\n",
    "from sklearn.tree import export_graphviz\n",
    "# to export the tree structure\n",
    "from sklearn.tree.export import export_text\n",
    "# to save the log file name\n",
    "from datetime import datetime\n",
    "\n",
    "file_name_time = datetime.utcnow().strftime('%Y-%m-%d_%H_%M_%S_%f')[:-3]\n",
    "\n",
    "__file__ = 'Data_analysis_' + file_name_time\n",
    "\n",
    "# initializing the logger\n",
    "logging.basicConfig(filename=os.getcwd() + '/../Log_files/' + __file__ + '.log',\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s %(levelname)s %(message)s',\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    level=logging.DEBUG)\n",
    "\n",
    "logging.info(\"Initialized logger\")\n",
    "print(\"Initialized logger\")\n",
    "\n",
    "logging.info(\"File name time is \" + file_name_time)\n",
    "print(\"File name time is\", file_name_time)\n",
    "\n",
    "logging.info(\"#### Run started ####\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berna\\Google Drive\\Work\\Research_collab\\Github_repositories\\Vehicle_insurance_claim_fraud_detection\\Jupyter_notebooks\n"
     ]
    }
   ],
   "source": [
    "# get the current working directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File C:\\Users\\berna\\Google Drive\\Work\\Research_collab\\Github_repositories\\Vehicle_insurance_claim_fraud_detection\\Jupyter_notebooks/../Dataset/fraud_oracle.csv does not exist: 'C:\\\\Users\\\\berna\\\\Google Drive\\\\Work\\\\Research_collab\\\\Github_repositories\\\\Vehicle_insurance_claim_fraud_detection\\\\Jupyter_notebooks/../Dataset/fraud_oracle.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mData_analysis_2023-09-22_19_43_46_437\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# reading the dataset stored as CSV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/../Dataset/fraud_oracle.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# printing the first 5 rows of the dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File C:\\Users\\berna\\Google Drive\\Work\\Research_collab\\Github_repositories\\Vehicle_insurance_claim_fraud_detection\\Jupyter_notebooks/../Dataset/fraud_oracle.csv does not exist: 'C:\\\\Users\\\\berna\\\\Google Drive\\\\Work\\\\Research_collab\\\\Github_repositories\\\\Vehicle_insurance_claim_fraud_detection\\\\Jupyter_notebooks/../Dataset/fraud_oracle.csv'"
     ]
    }
   ],
   "source": [
    "# reading the dataset stored as CSV\n",
    "data = pd.read_csv(os.getcwd() + '/../Dataset/fraud_oracle.csv')\n",
    "\n",
    "# printing the first 5 rows of the dataframe\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Showing class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the value counts\n",
    "print(data['FraudFound_P'].value_counts())\n",
    "\n",
    "print('\\nNo. of records:', data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the value counts\n",
    "vc = data.FraudFound_P.value_counts()\n",
    "\n",
    "# convert the value counts to a list\n",
    "list_vc = vc.tolist()\n",
    "list_vc\n",
    "\n",
    "# convert the value counts to another list\n",
    "vc_numbers_list = vc.to_list()\n",
    "\n",
    "# get the total number of claims in the dataset\n",
    "total = data.shape[0]\n",
    "\n",
    "# convert the value counts to percentage of the total number of claims\n",
    "list_vc = [round(l/total * 100, 2) for l in list_vc]\n",
    "list_vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the labels\n",
    "s = vc.index.tolist()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting new labels for plotting purposes only\n",
    "s = ['Non_fraudulent', 'Fraudulent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the x-axis and printing it\n",
    "yax = [text + ', ' + str(occ) + ', ' + str(perc) + '%' for text, occ, perc in zip(s, vc_numbers_list, list_vc)]\n",
    "yax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks = yax\n",
    "data_for_plotting = list_vc\n",
    "ind = np.arange(len(data_for_plotting))\n",
    "fig = plt.figure(tight_layout=True) # need tight_layout to make everything fit\n",
    "ax = plt.subplot(111)\n",
    "ax.bar(ind, data_for_plotting, 0.3, align='center', label=\"%\")\n",
    "# plt.legend() - legend is useless in this plot\n",
    "plt.title('Fradulent and non-fradulent vehicle insurance claims')\n",
    "y_pos = range(len(yax))\n",
    "plt.xticks(y_pos, yax)\n",
    "plt.draw()  # this is needed because get_window_extent needs a renderer to work\n",
    "plt.savefig(os.getcwd() + '/../Plots/claims_labels' + file_name_time + '.png')\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Identifying outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://careerfoundry.com/en/blog/data-analytics/how-to-find-outliers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some info about certain data\n",
    "data.describe()[['Age', 'DriverRating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This exercise was of not much help as the majority of the features are categorical, hence, we proceed with further analysis of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trying to identify concept drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first perform some pre-processing so that certain categorical features are converted to integer for sorting purposes\n",
    "\n",
    "# get each unique value of each column to be mapped\n",
    "print(data.MonthClaimed.unique())\n",
    "print(data.DayOfWeekClaimed.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the above cell we notice that there are certain errors in the data, 0 is not supposed to appear\n",
    "# thus we check how many records there are which have 0 under any of the two above mentioned columns and we remove them\n",
    "\n",
    "print('MonthClaimed:', data['MonthClaimed'].value_counts()['0'])\n",
    "print('DayOfWeekClaimed:', data['DayOfWeekClaimed'].value_counts()['0'])\n",
    "\n",
    "print('\\nNo. of records before deletion:', data.shape[0])\n",
    "\n",
    "# store new data for sorting to avoid mix ups\n",
    "data_sorted = data\n",
    "\n",
    "# remove the records with the invalid value under the specified columns\n",
    "# parameter inplace is set to False (also by default) so that we return a copy of the modified dataframe and store it\n",
    "# info retrieved from the official Pandas documentation - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html\n",
    "data_sorted = data_sorted.drop(data_sorted[data_sorted['MonthClaimed'] == '0'].index, inplace = False)\n",
    "data_sorted = data_sorted.drop(data_sorted[data_sorted['DayOfWeekClaimed'] == '0'].index, inplace = False)\n",
    "\n",
    "print('\\nNo. of records after deletion:', data_sorted.shape[0])\n",
    "\n",
    "# record removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert month and day of the week to integers for sorting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will map the months and days columns to an equivalent integer for sorting purposes\n",
    "# create dictionaries - https://www.geeksforgeeks.org/replacing-strings-with-numbers-in-python-for-data-analysis/\n",
    "\n",
    "MonthClaimed_dict = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6, 'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}\n",
    "DayOfWeekClaimed_dict = {'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5, 'Saturday': 6, 'Sunday': 7}\n",
    "\n",
    "# iterate the column which requires the mapping in the dataframe, get the current iterated month, fetch it in the dictionary, retrieve the mapping from the dictionary and place it in\n",
    "# a new column in the same dataset\n",
    "data_sorted['MonthClaimed_int'] = [MonthClaimed_dict[month] for month in data_sorted.MonthClaimed]\n",
    "data_sorted['DayOfWeekClaimed_int'] = [DayOfWeekClaimed_dict[day] for day in data_sorted.DayOfWeekClaimed]\n",
    "\n",
    "data_sorted.head()\n",
    "\n",
    "# mapping applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now sort the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store a new data set which is sorted first with Year -> MonthClaimed -> WeekOfMonthClaimed -> DayOfWeekClaimed (all in ascending order)\n",
    "data_sorted = data_sorted.sort_values(['Year', 'MonthClaimed_int', 'WeekOfMonthClaimed', 'DayOfWeekClaimed_int'], ascending=[True, True, True, True], inplace = False)\n",
    "\n",
    "# reset the index\n",
    "data_sorted = data_sorted.reset_index(drop=True)\n",
    "\n",
    "# store the dataset as CSV\n",
    "data_sorted.to_csv(os.getcwd() + '\\\\..\\\\Dataset\\\\fraud_oracle_sorted_' + file_name_time + '.csv')\n",
    "\n",
    "data_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new data frame and store the years and month in it (at this stage they are now sorted)\n",
    "dates = pd.DataFrame()\n",
    "dates['Year'] = data_sorted['Year']\n",
    "dates['MonthClaimed'] = data_sorted['MonthClaimed']\n",
    "\n",
    "# drop any duplicates\n",
    "dates = dates.drop_duplicates()\n",
    "\n",
    "# reset the dataframe index\n",
    "dates = dates.reset_index(drop = True)\n",
    "\n",
    "dates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we collect the number of fraudulent claims per year, and then month, so that we can plot fraudulent claims per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_claims = []\n",
    "\n",
    "# iterate over each year and month to get the fraudulent claims per month\n",
    "for year, month in zip(dates['Year'], dates['MonthClaimed']):\n",
    "    # append the total no of fraudulent claims for a certain month\n",
    "    fraud_claims.append(len(data_sorted[(data_sorted['Year'] == year) & (data_sorted['MonthClaimed'] == month) & (data_sorted['FraudFound_P'] == 1)].index))\n",
    "\n",
    "# add the collected data to the dataframe storing the year and month\n",
    "dates['fraud_claims'] = fraud_claims\n",
    "\n",
    "# for verification purposes\n",
    "print('Total no. of fraudulent claims:', np.sum(fraud_claims))\n",
    "\n",
    "print(dates.head())\n",
    "\n",
    "dates.to_csv(os.getcwd() + '/../Stats/fraud_claims_per_month' + file_name_time + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the fraudulent claims per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,10))\n",
    "\n",
    "# set the x-axis\n",
    "x_axis = [str(year) + ' ' + month for year, month in zip(dates['Year'], dates['MonthClaimed'])]\n",
    "\n",
    "plt.plot(x_axis, dates['fraud_claims'])\n",
    "plt.title(\"No. of fraudulent claims per month, no. of fraudulent claims vs month\")\n",
    "plt.savefig(os.getcwd() + '/../Plots/fraudulent_claims_per_month' + file_name_time + '.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Although we have a very small amount of data, we can see that the data is not stationary. Thus, we continue investigating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here onwards we build decision trees and check their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these settings were chosen arbitrarily\n",
    "window_size = 5000\n",
    "step = 1000\n",
    "\n",
    "# forgot the source of this equation\n",
    "num_of_iterations = math.ceil(((len(data_sorted.index)-window_size)/step)+1)\n",
    "\n",
    "print(num_of_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_number(text):\n",
    "    text_after = text.split(\"|--- \",1)[1]\n",
    "    number = text_after.split(\" \", 1)[0]\n",
    "    return int(number.split(\"_\",1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_level(text):\n",
    "    return text.count('|   ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a column consisting of the Year and MonthClaimed joined into one single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted['YearMonthClaimed'] = [str(year) + ' ' + month for year, month in zip(data_sorted['Year'], data_sorted['MonthClaimed'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we get the number of claims per generated window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe for the Year and MonthClaimed frequency count\n",
    "dates_freq = pd.DataFrame({'YearMonthClaimed': data_sorted.YearMonthClaimed.unique()})\n",
    "\n",
    "# also create two seperate columns for the Year and MonthClaimed\n",
    "dates_freq['Year'] = dates.Year\n",
    "dates_freq['MonthClaimed'] = dates.MonthClaimed\n",
    "\n",
    "# another dataframe to use during the window generation process\n",
    "data_to_use = data_sorted\n",
    "\n",
    "# retrieve the records and do frequency counts for Year and MonthClaimed per window\n",
    "for i in range(0, num_of_iterations):\n",
    "    \n",
    "    # this dataframe is used to generate the windows\n",
    "    to_use = pd.DataFrame()\n",
    "    \n",
    "    if len(data_to_use.index) >= window_size:\n",
    "    \n",
    "        # retrieve the first number of records using the window size\n",
    "        to_use = data_to_use.head(window_size)\n",
    "        \n",
    "        # remove the first n (step size) rows\n",
    "        data_to_use = data_to_use.iloc[step:]\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # retrieve the first number of records using the size of the dataframe since it's the last window\n",
    "        to_use = data_to_use.head(len(data_to_use.index))\n",
    "    \n",
    "    # get the frequency counts of the Year and MonthClaimed\n",
    "    dates_freq['tree_' + str(i)] = dates_freq['YearMonthClaimed'].map(to_use['YearMonthClaimed'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the YearMonthClaimed frequency per window tree to csv\n",
    "dates_freq.to_csv(os.getcwd() + '/../Stats/dates_freq' + file_name_time + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the column names\n",
    "data_sorted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify which features to keep\n",
    "# removed RepNumber, PolicyNumber, MonthClaimed_int, DayOfWeekClaimed_int\n",
    "featuresToKeep = ['Month', 'WeekOfMonth', 'DayOfWeek','Make','AccidentArea', \n",
    "                 'DayOfWeekClaimed','MonthClaimed','WeekOfMonthClaimed', \n",
    "                 'Sex','MaritalStatus','Age', \n",
    "                 'Fault','PolicyType','VehicleCategory',\n",
    "                 'VehiclePrice','Deductible','DriverRating', \n",
    "                 'Days_Policy_Accident','Days_Policy_Claim','PastNumberOfClaims','AgeOfVehicle', \n",
    "                 'AgeOfPolicyHolder','PoliceReportFiled','WitnessPresent',\n",
    "                 'AgentType', 'NumberOfSuppliments','AddressChange_Claim', \n",
    "                 'NumberOfCars', 'BasePolicy', 'Year', 'FraudFound_P']\n",
    "\n",
    "data_sorted = data_sorted[featuresToKeep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that applies label encoding to the string categorical features\n",
    "def apply_label_encoding(data):\n",
    "    \n",
    "    # creating instance of labelencoder\n",
    "    labelencoder = LabelEncoder()\n",
    "    \n",
    "    # Assigning numerical values and storing in another column\n",
    "    data['Month'] = labelencoder.fit_transform(data['Month'])\n",
    "    data['DayOfWeek'] = labelencoder.fit_transform(data['DayOfWeek'])\n",
    "    data['Make'] = labelencoder.fit_transform(data['Make'])\n",
    "    data['AccidentArea'] = labelencoder.fit_transform(data['AccidentArea'])\n",
    "    data['DayOfWeekClaimed'] = labelencoder.fit_transform(data['DayOfWeekClaimed'])\n",
    "    data['MonthClaimed'] = labelencoder.fit_transform(data['MonthClaimed'])\n",
    "    data['Sex'] = labelencoder.fit_transform(data['Sex'])\n",
    "    data['MaritalStatus'] = labelencoder.fit_transform(data['MaritalStatus'])\n",
    "    data['Fault'] = labelencoder.fit_transform(data['Fault'])\n",
    "    data['PolicyType'] = labelencoder.fit_transform(data['PolicyType'])\n",
    "    data['VehicleCategory'] = labelencoder.fit_transform(data['VehicleCategory'])\n",
    "    data['VehiclePrice'] = labelencoder.fit_transform(data['VehiclePrice'])\n",
    "    data['Days_Policy_Accident'] = labelencoder.fit_transform(data['Days_Policy_Accident'])\n",
    "    data['Days_Policy_Claim'] = labelencoder.fit_transform(data['Days_Policy_Claim'])\n",
    "    data['PastNumberOfClaims'] = labelencoder.fit_transform(data['PastNumberOfClaims']) # has certain cells as null\n",
    "    data['AgeOfVehicle'] = labelencoder.fit_transform(data['AgeOfVehicle'])\n",
    "    data['AgeOfPolicyHolder'] = labelencoder.fit_transform(data['AgeOfPolicyHolder'])\n",
    "    data['PoliceReportFiled'] = labelencoder.fit_transform(data['PoliceReportFiled'])\n",
    "    data['WitnessPresent'] = labelencoder.fit_transform(data['WitnessPresent'])\n",
    "    data['AgentType'] = labelencoder.fit_transform(data['AgentType'])\n",
    "    data['NumberOfSuppliments'] = labelencoder.fit_transform(data['NumberOfSuppliments'])\n",
    "    data['AddressChange_Claim'] = labelencoder.fit_transform(data['AddressChange_Claim'])\n",
    "    data['NumberOfCars'] = labelencoder.fit_transform(data['NumberOfCars'])\n",
    "    data['BasePolicy'] = labelencoder.fit_transform(data['BasePolicy'])\n",
    "    \n",
    "    \n",
    "    return data\n",
    "\n",
    "# apply the label encoding to the selected categorical features\n",
    "data_sorted = apply_label_encoding(data_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df = data_sorted\n",
    "\n",
    "df = data_sorted\n",
    "\n",
    "# create a dataframe which will store the feature levels\n",
    "index = df.columns.tolist()\n",
    "index.remove('FraudFound_P')\n",
    "feature_levels = pd.DataFrame({'features': index})\n",
    "\n",
    "# create a column to store the number of NAs for each feature\n",
    "feature_levels['all_NAs'] = False\n",
    "feature_levels['no_of_NAs'] = 0\n",
    "feature_levels['no_of_times_root'] = 0\n",
    "\n",
    "logging.info(\"Will generate the windows\")\n",
    "print(\"Will generate the windows...\")\n",
    "\n",
    "# lists for number of nodes and tree number\n",
    "no_of_nodes = []\n",
    "tree_depth = []\n",
    "tree_column = []\n",
    "\n",
    "# to store the window sizes\n",
    "window_sizes = []\n",
    "\n",
    "# retrieve the records and create decision trees\n",
    "for i in range(0, num_of_iterations):\n",
    "\n",
    "    to_use = pd.DataFrame()\n",
    "    \n",
    "    if len(df.index) >= window_size:\n",
    "    \n",
    "        # retrieve the first number of records using the window size\n",
    "        to_use = df.head(window_size)\n",
    "        \n",
    "        # remove the first n (step size) rows\n",
    "        df = df.iloc[step:]\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # retrieve the first number of records using the size of the dataframe since it's the last window\n",
    "        to_use = df.head(len(df.index))\n",
    "    \n",
    "    # append the size of the windows\n",
    "    window_sizes.append(len(to_use.index))\n",
    "        \n",
    "    # print(\"Size of window is\", str(len(to_use.index)))     \n",
    "    \n",
    "    # initialize the decision tree\n",
    "    dt = DecisionTreeClassifier(criterion='entropy')\n",
    "    \n",
    "    #dt = DecisionTreeClassifier(criterion='entropy', splitter='random',\n",
    "                                #max_depth=10, min_samples_split=2,\n",
    "                                #min_samples_leaf=10,\n",
    "                                #max_leaf_nodes=None, class_weight='balanced')\n",
    "    \n",
    "    # split the label and rest of the data\n",
    "    y = to_use['FraudFound_P']\n",
    "    X = to_use.drop('FraudFound_P', axis=1)\n",
    "\n",
    "    # split the sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle = False)\n",
    "    \n",
    "    # train the decision tree\n",
    "    dt = dt.fit(X_train, y_train)\n",
    "\n",
    "    logging.info(\"Decision tree \" + str(i) + \" created\")\n",
    "\n",
    "    export_graphviz(dt, out_file=os.getcwd() + '/../Dot_files/tree_' + str(i) + '_' + file_name_time + '.dot', feature_names=X_train.columns,\n",
    "                    class_names=['0', '1'],\n",
    "                    rounded=True, proportion=False, precision=2, filled=True)\n",
    "    \n",
    "    # new df for the most important features\n",
    "    most_important_features = pd.DataFrame(columns = ['tree_number', 'feature', 'feature_number'])\n",
    "    \n",
    "    # top 5 important features\n",
    "    # counter to rank the top 5 most important features\n",
    "    c = 0\n",
    "    logging.info('The top 5 most importand features:')\n",
    "    for importance, name in sorted(zip(dt.feature_importances_, X_train.columns), reverse=True)[:len(df.columns)]:\n",
    "        logging.info(name + \" \" + str(importance))\n",
    "        c += 1\n",
    "        most_important_features.loc[len(most_important_features.index)] = ['tree_' + str(i), name, c]\n",
    "    \n",
    "    # now save the CSV\n",
    "    most_important_features.to_csv(os.getcwd() + '/../Tree_structures/most_important_features' + file_name_time + '.csv')\n",
    "    \n",
    "    logging.info(\"Number of nodes for tree \" + str(i) + \" is \" + str(dt.tree_.node_count))\n",
    "    \n",
    "    # append the number of nodes\n",
    "    no_of_nodes.append(dt.tree_.node_count)\n",
    "    tree_depth.append(dt.tree_.max_depth)\n",
    "    tree_column.append('tree_' + str(i))\n",
    "    \n",
    "    # get the tree rules\n",
    "    tree_rules = export_text(dt)\n",
    "    tree_rules_with_feature_names = export_text(dt, feature_names = X_train.columns.tolist())\n",
    "    \n",
    "    text_file = open(os.getcwd() + \"/../Tree_structures/tree_\" + str(i) + \"_structure\" + file_name_time + \".txt\", \"w\")\n",
    "    text_file.write(tree_rules_with_feature_names)\n",
    "    text_file.close()\n",
    "    \n",
    "    # create a column for the current tree\n",
    "    feature_levels['tree_' + str(i)] = 'NA'\n",
    "    \n",
    "    # check each line\n",
    "    for line in tree_rules.splitlines():\n",
    "        if 'class' not in line and 'truncated' not in line:\n",
    "            level = get_feature_level(line)\n",
    "            feature_number = get_feature_number(line)\n",
    "            \n",
    "            # add the feature level\n",
    "            if feature_levels.at[feature_number, 'tree_' + str(i)]== 'NA':\n",
    "                feature_levels.loc[feature_number, 'tree_' + str(i)] = ',' + str(level)\n",
    "            elif str(level) not in feature_levels.iloc[feature_number]['tree_' + str(i)]:\n",
    "                feature_levels.loc[feature_number, 'tree_' + str(i)] = feature_levels.iloc[feature_number]['tree_' + str(i)] + ',' + str(level)\n",
    "        \n",
    "    logging.info(\"\\n\")\n",
    "    \n",
    "print(\"Trees generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the number of nodes and depth of each tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the number of nodes for each tree\n",
    "plt.figure(figsize=(35,10))\n",
    "plt.title('no_of_nodes')\n",
    "plt.plot(no_of_nodes, \"-b\")\n",
    "plt.savefig(os.getcwd() + '/../Plots/no_of_nodes_per_tree' + file_name_time + '.png')\n",
    "\n",
    "plt.figure(figsize=(35,10))\n",
    "plt.title('tree_depth')\n",
    "plt.plot(tree_depth, \"-r\")\n",
    "plt.savefig(os.getcwd() + '/../Plots/tree_depth_per_tree' + file_name_time + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the number of times that a feature did not appear in a tree, and the number of times a feature was the root of the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute further information\n",
    "feature_levels_list = feature_levels.values.tolist()\n",
    "\n",
    "# store a counter to access the dataframe index\n",
    "count = 0\n",
    "\n",
    "# iterate each row\n",
    "for row in feature_levels_list:\n",
    "    # count the number of NAs in a row\n",
    "    no_of_NAs = row.count('NA')\n",
    "    feature_levels.loc[count, 'no_of_NAs'] = no_of_NAs\n",
    "    \n",
    "    # find how much times feature was a root\n",
    "    no_of_times_root = 0\n",
    "    for item in row[4:]:\n",
    "        if item.count(',0,') > 0:\n",
    "            no_of_times_root += 1\n",
    "    feature_levels.loc[count, 'no_of_times_root'] = no_of_times_root    \n",
    "    \n",
    "    # mark if all NAs\n",
    "    if no_of_NAs == len(row) - 4:\n",
    "        print(feature_levels.loc[count, 'features'], 'did not appear once in a tree')\n",
    "        feature_levels.loc[count, 'all_NAs'] = True\n",
    "    \n",
    "    # increase counter\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe with the feature levels\n",
    "feature_levels.to_csv(os.getcwd() + '/../Tree_structures/feature_levels' + file_name_time + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe with the number of nodes\n",
    "df_no_of_nodes = pd.DataFrame({'tree': tree_column, 'no_of_nodes': no_of_nodes, 'tree_depth': tree_depth, 'window_sizes': window_sizes})\n",
    "df_no_of_nodes.to_csv(os.getcwd() + '/../Tree_structures/tree_stats' + file_name_time + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"#### Run finished ####\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
